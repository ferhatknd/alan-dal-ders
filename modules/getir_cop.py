"""
MEB Ã‡Ã–P (Ã‡erÃ§eve Ã–ÄŸretim ProgramÄ±) Veri Ã‡ekme ModÃ¼lÃ¼

Bu modÃ¼l MEB sitesinden HTML parsing yaparak Ã‡Ã–P linklerini Ã§eker ve
PDF dosyalarÄ±nÄ± merkezi cache sistemine indirir.

Sorumluluklar:
- HTML scraping ve alan listesi Ã§Ä±karma
- PDF URL'lerini bulma ve organize etme  
- utils.py Ã¼zerinden merkezi indirme
- SÄ±nÄ±f bazlÄ± paralel istek yÃ¶netimi
- Cache kontrolÃ¼ ve metadata yÃ¶netimi
"""

import requests
import os
import json
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple
from .utils import download_and_cache_pdf, normalize_to_title_case_tr


def get_cop_data_for_class(sinif_kodu: str) -> Tuple[str, Dict[str, Dict[str, str]]]:
    """
    Belirli bir sÄ±nÄ±f iÃ§in Ã‡Ã–P verilerini Ã§ek
    
    Args:
        sinif_kodu: SÄ±nÄ±f kodu (9, 10, 11, 12)
    
    Returns:
        Tuple[sinif_kodu, alan_dict]: (sÄ±nÄ±f, {alan_adi: {link, guncelleme_yili}})
    """
    try:
        url = "https://meslek.meb.gov.tr/cercevelistele.aspx"
        params = {
            'sinif_kodu': sinif_kodu,
            'kurum_id': '1'
        }
        
        response = requests.get(url, params=params, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        alanlar = {}
        
        # Alan kartlarÄ±nÄ± bul
        alan_columns = soup.find_all('div', class_='col-lg-3')
        
        for column in alan_columns:
            try:
                # Link ve alan bilgisini Ã§Ä±kar
                link_tag = column.find('a', href=True)
                if not link_tag:
                    continue
                
                # Alan adÄ±nÄ± img alt attribute'Ä±ndan al
                img_tag = link_tag.find('img', alt=True)
                if not img_tag:
                    continue
                
                alan_adi = img_tag.get('alt', '').strip()
                if not alan_adi:
                    continue

                # HatalÄ± "alan adÄ±" olarak algÄ±lanan metinleri filtrele
                invalid_keywords = [
                    "Ã‡ERÃ‡EVE Ã–ÄRETÄ°M PROGRAMI",
                    "Ã–ÄRETÄ°M PROGRAMININ AMAÃ‡LARI",
                    "LOGO",
                    "MEB"
                ]
                # Ã‡ok uzun veya anlamsÄ±z metinleri de filtrele (Ã¶rn: ... iÃ§erenler)
                if any(keyword in alan_adi.upper() for keyword in invalid_keywords) or "..." in alan_adi or len(alan_adi) > 100:
                    continue # Bu geÃ§erli bir alan adÄ± deÄŸil, atla.

                
                # Ã‡Ã–P PDF linkini al
                href = link_tag.get('href', '').strip()
                if not href.endswith('.pdf') or 'upload/cop' not in href:
                    continue
                
                full_link = requests.compat.urljoin(response.url, href)
                
                # GÃ¼ncelleme yÄ±lÄ±nÄ± ribbon'dan al
                guncelleme_yili = None
                ribbon = column.find('div', class_='ribbon')
                if ribbon:
                    span_tag = ribbon.find('span')
                    if span_tag:
                        guncelleme_yili = span_tag.get_text(strip=True)
                
                alanlar[alan_adi] = {
                    'link': full_link,
                    'guncelleme_yili': guncelleme_yili or 'Bilinmiyor'
                }
                
            except Exception as e:
                print(f"Alan iÅŸleme hatasÄ± (sÄ±nÄ±f {sinif_kodu}): {e}")
                continue
        
        return sinif_kodu, alanlar
        
    except Exception as e:
        print(f"Ã‡Ã–P Ã§ekme hatasÄ± (sÄ±nÄ±f {sinif_kodu}): {e}")
        return sinif_kodu, {}


def getir_cop_links(siniflar: List[str] = ["9", "10", "11", "12"]) -> Dict[str, Any]:
    """
    MEB sitesinden Ã‡Ã–P (Ã‡erÃ§eve Ã–ÄŸretim ProgramÄ±) linklerini Ã§eker.
    
    Args:
        siniflar: Ã‡ekilecek sÄ±nÄ±f seviyeleri (default: ["9", "10", "11", "12"])
    
    Returns:
        dict: {
            "cop_data": {sÄ±nÄ±f: {alan_adi: {link, guncelleme_yili}}},
            "alan_ids": {sÄ±nÄ±f: [{id, isim}]}
        }
    """
    cop_data = {}
    alan_ids_data = {}
    
    # Paralel olarak tÃ¼m sÄ±nÄ±flarÄ± iÅŸle
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(get_cop_data_for_class, sinif): sinif for sinif in siniflar}
        
        for future in as_completed(futures):
            sinif_kodu, alanlar = future.result()
            
            if alanlar:
                cop_data[sinif_kodu] = alanlar
                
                # Alan ID'leri iÃ§in basit indexing
                alan_ids_data[sinif_kodu] = [
                    {"id": str(i+1), "isim": alan_adi} 
                    for i, alan_adi in enumerate(alanlar.keys())
                ]
                
                print(f"âœ… SÄ±nÄ±f {sinif_kodu}: {len(alanlar)} alan bulundu")
            else:
                print(f"âŒ SÄ±nÄ±f {sinif_kodu}: Veri bulunamadÄ±")
    
    return {
        "cop_data": cop_data,
        "alan_ids": alan_ids_data
    }


def download_cop_pdfs(alan_list: List[Dict[str, str]], cache: bool = True, alan_id_mapping: Optional[Dict[str, str]] = None) -> Dict[str, str]:
    """
    Ã‡Ã–P PDF'lerini utils.py Ã¼zerinden merkezi cache sistemine indirir.
    
    Args:
        alan_list: [{"alan_adi": "...", "link": "...", "sinif": "...", "year": "..."}] formatÄ±nda alan listesi
        cache: True ise kalÄ±cÄ± cache, False ise geÃ§ici indirme
        alan_id_mapping: {"alan_adi": "id"} formatÄ±nda ID mapping'i (opsiyonel)
    
    Returns:
        Dict[str, str]: {"alan_adi_sinif": "indirilen_dosya_yolu"} formatÄ±nda sonuÃ§
    """
    if not alan_list:
        return {}
    
    downloaded_files = {}
    
    for alan_info in alan_list:
        alan_adi = alan_info.get("alan_adi", "")
        pdf_url = alan_info.get("link", "")
        sinif = alan_info.get("sinif", "")
        year = alan_info.get("year", "")
        
        if not alan_adi or not pdf_url:
            continue
        
        try:
            # Ek bilgi oluÅŸtur (sÄ±nÄ±f ve yÄ±l)
            additional_info = f"{sinif}_sinif"
            if year and year != "Bilinmiyor":
                additional_info += f"_{year}"
            
            # Alan ID'si varsa kullan
            alan_id = alan_id_mapping.get(alan_adi) if alan_id_mapping else None
            
            if cache:
                # KalÄ±cÄ± cache
                file_path = download_and_cache_pdf(
                    url=pdf_url,
                    cache_type="cop",
                    alan_adi=alan_adi,
                    additional_info=additional_info,
                    alan_id=alan_id
                )
            else:
                # GeÃ§ici indirme
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    response = requests.get(pdf_url, timeout=30)
                    response.raise_for_status()
                    tmp_file.write(response.content)
                    file_path = tmp_file.name
            
            if file_path:
                key = f"{alan_adi}_{sinif}"
                downloaded_files[key] = file_path
                print(f"âœ… Ä°ndirildi: {alan_adi} ({sinif}. sÄ±nÄ±f)")
            else:
                print(f"âŒ Ä°ndirme baÅŸarÄ±sÄ±z: {alan_adi} ({sinif}. sÄ±nÄ±f)")
                
        except Exception as e:
            print(f"âŒ Ä°ndirme hatasÄ± ({alan_adi}): {e}")
    
    return downloaded_files


def get_cop_metadata(save_to_file: bool = True) -> Dict[str, Any]:
    """
    Ã‡Ã–P verilerinin metadata'sÄ±nÄ± toplar ve opsiyonel olarak dosyaya kaydeder.
    
    Args:
        save_to_file: True ise metadata'yÄ± JSON dosyasÄ±na kaydet
    
    Returns:
        Dict: Ã‡Ã–P metadata'sÄ±
    """
    print("ğŸ“Š Ã‡Ã–P metadata'sÄ± toplanÄ±yor...")
    
    # TÃ¼m sÄ±nÄ±flar iÃ§in veri Ã§ek
    cop_data = getir_cop_links()
    
    # Ä°statistikler
    total_areas = 0
    total_files = 0
    sinif_stats = {}
    
    for sinif, alanlar in cop_data["cop_data"].items():
        alan_count = len(alanlar)
        total_areas += alan_count
        total_files += alan_count  # Her alan iÃ§in bir PDF
        
        sinif_stats[sinif] = {
            "alan_sayisi": alan_count,
            "alanlar": list(alanlar.keys())
        }
    
    metadata = {
        "toplam_alan_sayisi": total_areas,
        "toplam_pdf_sayisi": total_files,
        "sinif_istatistikleri": sinif_stats,
        "cop_verileri": cop_data["cop_data"],
        "alan_ids": cop_data["alan_ids"],
        "olusturma_tarihi": __import__('datetime').datetime.now().isoformat()
    }
    
    if save_to_file:
        os.makedirs("data/cop", exist_ok=True)
        metadata_file = "data/cop/cop_metadata.json"
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Metadata kaydedildi: {metadata_file}")
    
    print(f"ğŸ“ˆ Toplam {total_areas} alan, {total_files} PDF bulundu")
    return metadata


def validate_cop_links(cop_data: Dict[str, Dict[str, Dict[str, str]]]) -> Dict[str, List[str]]:
    """
    Ã‡Ã–P linklerinin geÃ§erliliÄŸini kontrol eder.
    
    Args:
        cop_data: getir_cop_links()'den dÃ¶nen cop_data kÄ±smÄ±
    
    Returns:
        Dict: {"gecerli": [...], "gecersiz": [...], "errors": [...]}
    """
    gecerli_linkler = []
    gecersiz_linkler = []
    hatalar = []
    
    print("ğŸ” Ã‡Ã–P linkleri doÄŸrulanÄ±yor...")
    
    for sinif, alanlar in cop_data.items():
        for alan_adi, alan_info in alanlar.items():
            pdf_url = alan_info.get("link", "")
            
            if not pdf_url:
                gecersiz_linkler.append(f"{alan_adi} ({sinif}): URL boÅŸ")
                continue
            
            try:
                # HEAD request ile dosya varlÄ±ÄŸÄ±nÄ± kontrol et
                response = requests.head(pdf_url, timeout=10)
                
                if response.status_code == 200:
                    content_type = response.headers.get('Content-Type', '')
                    if 'pdf' in content_type.lower():
                        gecerli_linkler.append(f"{alan_adi} ({sinif})")
                    else:
                        gecersiz_linkler.append(f"{alan_adi} ({sinif}): PDF deÄŸil ({content_type})")
                else:
                    gecersiz_linkler.append(f"{alan_adi} ({sinif}): HTTP {response.status_code}")
                    
            except Exception as e:
                hatalar.append(f"{alan_adi} ({sinif}): {str(e)}")
    
    result = {
        "gecerli": gecerli_linkler,
        "gecersiz": gecersiz_linkler,
        "errors": hatalar
    }
    
    print(f"âœ… GeÃ§erli: {len(gecerli_linkler)}")
    print(f"âŒ GeÃ§ersiz: {len(gecersiz_linkler)}")
    print(f"âš ï¸ Hata: {len(hatalar)}")
    
    return result


# Geriye uyumluluk iÃ§in eski fonksiyon adÄ±
def getir_cop(siniflar: List[str] = ["9", "10", "11", "12"]) -> Dict[str, Any]:
    """
    Geriye uyumluluk wrapper'Ä± - getir_cop_links() fonksiyonunu Ã§aÄŸÄ±rÄ±r.
    
    DEPRECATED: Bunun yerine getir_cop_links() kullanÄ±n.
    """
    import warnings
    warnings.warn(
        "getir_cop() fonksiyonu deprecated. getir_cop_links() kullanÄ±n.",
        DeprecationWarning,
        stacklevel=2
    )
    return getir_cop_links(siniflar)


if __name__ == "__main__":
    # Test amaÃ§lÄ± Ã§alÄ±ÅŸtÄ±rma
    print("ğŸš€ Ã‡Ã–P veri Ã§ekme testi baÅŸlatÄ±lÄ±yor...")
    
    # Sadece 9. sÄ±nÄ±f test et
    cop_data = getir_cop_links(["9"])
    
    if cop_data["cop_data"]:
        print("\nğŸ“‹ Bulunan alanlar:")
        for sinif, alanlar in cop_data["cop_data"].items():
            print(f"\n{sinif}. SÄ±nÄ±f ({len(alanlar)} alan):")
            for alan_adi, info in alanlar.items():
                print(f"  - {alan_adi} [{info['guncelleme_yili']}]")
        
        # Ä°lk 2 alanÄ± test indirme
        first_areas = list(cop_data["cop_data"]["9"].items())[:2]
        test_list = []
        
        for alan_adi, info in first_areas:
            test_list.append({
                "alan_adi": alan_adi,
                "link": info["link"],
                "sinif": "9",
                "year": info["guncelleme_yili"]
            })
        
        print(f"\nâ¬‡ï¸ Test indirme ({len(test_list)} alan)...")
        downloaded = download_cop_pdfs(test_list, cache=True)
        
        print(f"\nâœ… {len(downloaded)} dosya indirildi")
        for key, path in downloaded.items():
            print(f"  - {key}: {path}")
    else:
        print("âŒ HiÃ§ veri bulunamadÄ±")