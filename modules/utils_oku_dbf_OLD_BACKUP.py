"""
modules/utils_oku_dbf.py
========================
Bu modÃ¼l extract_olcme.py'den kopyalanan stabil DBF okuma fonksiyonlarÄ±nÄ± iÃ§erir.
"""

import fitz  # PyMuPDF
import re
import os
import glob
import unicodedata

# ===========================
# UTILITY CLASSES & HELPERS
# ===========================

# Metin iÅŸleme sÄ±nÄ±fÄ± - Normalizasyon iÅŸlemlerini Ã¶nbelleÄŸe alarak performansÄ± artÄ±rÄ±r
# PDF'den Ã§Ä±karÄ±lan metinleri normalize eder ve arama iÅŸlemlerini hÄ±zlandÄ±rÄ±r
class TextProcessor:
    """
    Text processing utility with caching to avoid repeated normalization
    """
    def __init__(self, text):
        self.original = text
        self.normalized = normalize_turkish_text(text) if text else ""
        self._cache = {}
    
    def find_normalized(self, pattern, start=0):
        """Find pattern in normalized text"""
        cache_key = f"{pattern}_{start}"
        if cache_key not in self._cache:
            pattern_norm = normalize_turkish_text(pattern) if pattern else ""
            self._cache[cache_key] = self.normalized.find(pattern_norm, start)
        return self._cache[cache_key]
    
    def normalize_text(self, text):
        """Normalize text using cached normalization function"""
        import re
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip())
    
    def get_section(self, start_idx, end_idx=None):
        """Get text section, returns both original and normalized"""
        if end_idx is None:
            end_idx = len(self.original)
        return {
            'original': self.original[start_idx:end_idx],
            'normalized': self.normalized[start_idx:end_idx]
        }

# Konu numarasÄ± pattern'lerini doÄŸrular ("1. ", "2. " gibi)
# Metinde beklenen sayÄ±da sÄ±ralÄ± konu numarasÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol eder
def validate_konu_patterns(text, konu_sayisi, start_pos=0, search_limit=1500):
    """
    Centralized pattern validation logic - eliminates code duplication
    
    Args:
        text (str): Text to search in
        konu_sayisi (int): Expected number of topics
        start_pos (int): Starting position
        search_limit (int): Limit search area
        
    Returns:
        int: Number of valid patterns found
    """
    found_numbers = 0
    search_area = text[start_pos:start_pos + search_limit] if start_pos + search_limit < len(text) else text[start_pos:]
    
    for rakam in range(1, konu_sayisi + 1):
        patterns = [f"{rakam}. ", f"{rakam} "]
        pattern_found = False
        
        for pattern in patterns:
            pos = search_area.find(pattern)
            if pos != -1:
                # Context-aware validation: gerÃ§ek madde baÅŸlÄ±ÄŸÄ± mÄ±?
                if is_valid_madde_baslik(search_area, pos, rakam):
                    pattern_found = True
                    break
        
        if pattern_found:
            found_numbers += 1
    
    return found_numbers

# BaÅŸlÄ±k eÅŸleÅŸtirme fonksiyonu - Normalize edilmiÅŸ metinde baÅŸlÄ±k pozisyonlarÄ±nÄ± bulur
# TÃ¼rkÃ§e karakter sorunlarÄ±nÄ± Ã§Ã¶zmek iÃ§in normalizasyon kullanÄ±r
def find_baslik_matches(baslik, content_processor, start_pos=0):
    """
    Find all matches of a header in content with normalized search
    
    Args:
        baslik (str): Header to search for
        content_processor (TextProcessor): Content with cached normalization
        start_pos (int): Starting position
        
    Returns:
        list: List of match positions
    """
    matches = []
    current_pos = start_pos
    baslik_normalized = normalize_turkish_text(baslik)
    
    while True:
        match_idx = content_processor.normalized.find(baslik_normalized, current_pos)
        if match_idx == -1:
            break
        matches.append(match_idx)
        current_pos = match_idx + 1
    
    return matches

# Tablo sÄ±nÄ±rlarÄ±nÄ± belirler - Ã¶ÄŸrenme birimi tablosunun baÅŸlangÄ±Ã§ ve bitiÅŸ noktalarÄ±nÄ± bulur
# TOPLAM kelimesinden baÅŸlayarak uygun baÅŸlÄ±klarÄ± arar ve stop kelimelerinde durdurur
def extract_table_boundaries(full_text):
    """
    Extract table start and end boundaries - centralized logic
    
    Returns:
        dict: Contains start_idx, end_idx, and content section
    """
    processor = TextProcessor(full_text)
    
    # Find TOPLAM reference point
    toplam_idx = processor.find_normalized("TOPLAM")
    if toplam_idx == -1:
        toplam_idx = 0
    
    # Find table headers
    table_headers = [
        "Ã–ÄRENME BÄ°RÄ°MÄ°", "KONULAR", "Ã–ÄRENME BÄ°RÄ°MÄ° KAZANIMLARI",
        "KAZANIM AÃ‡IKLAMLARI", "AÃ‡IKLAMALARI", "Ã–ÄRENME BÄ°RÄ°MÄ°/ÃœNÄ°TE"
    ]
    
    table_start_idx = None
    last_header_end = None
    
    for header in table_headers:
        idx = processor.find_normalized(header, toplam_idx)
        if idx != -1:
            header_end = idx + len(normalize_turkish_text(header))
            if last_header_end is None or header_end > last_header_end:
                last_header_end = header_end
                table_start_idx = header_end
    
    if table_start_idx is None:
        return None
    
    # Find table end using stop words
    stop_words = ["UYGULAMA", "FAALÄ°YET", "TEMRÄ°N", "DERSÄ°N", "DERSÄ N"]
    table_end_idx = len(full_text)
    
    for stop_word in stop_words:
        word_pattern = r'\\b' + re.escape(stop_word) + r'\\b'
        match = re.search(word_pattern, full_text[table_start_idx:])
        if match:
            stop_idx = table_start_idx + match.start()
            if stop_idx < table_end_idx:
                table_end_idx = stop_idx
    
    content = full_text[table_start_idx:table_end_idx].strip()
    content = re.sub(r'\\s+', ' ', content).strip()
    
    return {
        'start_idx': table_start_idx,
        'end_idx': table_end_idx,
        'content': content,
        'processor': TextProcessor(content)
    }

# BaÅŸlÄ±k eÅŸleÅŸmelerini bulur ve doÄŸrular - her baÅŸlÄ±k iÃ§in geÃ§erli pozisyonlarÄ± tespit eder
# Kazanm tablosundaki baÅŸlÄ±klarÄ± metinde bulur ve pattern doÄŸrulamasÄ± yapar
def find_header_matches_with_validation(boundaries, kazanim_data):
    """
    Find and validate header matches in content
    
    Returns:
        dict: Contains all_matched_headers and match statistics
    """
    all_matched_headers = []
    content_processor = boundaries['processor']
    
    for item in kazanim_data:
        baslik = item['title']
        konu_sayisi_int = item['count']
        baslik_cleaned = re.sub(r'\\s+', ' ', baslik.strip())
        
        # CONSOLIDATED: Use unified validation logic
        valid_matches = _validate_single_header_match(boundaries, baslik_cleaned, konu_sayisi_int, return_first_only=False)
        
        # Take first valid match if any found
        if valid_matches:
            all_matched_headers.append({
                'title': baslik,
                'position': valid_matches[0],  # Use first valid match
                'konu_sayisi': konu_sayisi_int
            })
    
    return all_matched_headers

# BaÅŸlÄ±k eÅŸleÅŸmelerini iÅŸler ve iÃ§erik Ã§Ä±karÄ±r - structured data formatÄ±nda sonuÃ§ Ã¼retir
# Her baÅŸlÄ±k iÃ§in konu listesini ve kazanÄ±mlarÄ± Ã§Ä±karÄ±r
def process_header_matches(boundaries, kazanim_data, all_matched_headers):
    """
    Process each header and extract content with structured data
    
    Returns:
        tuple: (header_match_info, formatted_content_parts, structured_konu_data)
    """
    header_match_info = "\\n"
    formatted_content_parts = []
    structured_konu_data = []
    content_processor = boundaries['processor']
    
    for i, item in enumerate(kazanim_data, 1):
        baslik = item['title']
        konu_sayisi_int = item['count']
        baslik_cleaned = re.sub(r'\\s+', ' ', baslik.strip())
        
        # Count valid matches for this header
        valid_matches = 0
        matches = find_baslik_matches(baslik_cleaned, content_processor)
        
        for match_idx in matches:
            after_baslik_pos = match_idx + len(baslik_cleaned)
            after_baslik = boundaries['content'][after_baslik_pos:]
            
            if konu_sayisi_int > 0:
                found_numbers = validate_konu_patterns(after_baslik, konu_sayisi_int)
                if found_numbers == konu_sayisi_int:
                    valid_matches += 1
        
        header_match_info += f"{i}-{baslik} ({konu_sayisi_int}) -> {valid_matches} eÅŸleÅŸme\\n"
        
        # Try alternative method if no matches found
        if valid_matches == 0 and konu_sayisi_int > 0:
            alternative_match = ex_ob_tablosu_konu_bulma_yedek_plan(
                boundaries['content'], baslik_cleaned, konu_sayisi_int
            )
            if alternative_match:
                valid_matches = 1
                header_match_info = header_match_info.replace(
                    f"{i}-{baslik} ({konu_sayisi_int}) -> 0 eÅŸleÅŸme\\n",
                    f"{i}-{baslik} ({konu_sayisi_int}) -> 1 eÅŸleÅŸme (alternatif)\\n"
                )
        
        # Process first valid match for content extraction
        if valid_matches > 0:
            first_match = find_first_valid_match(boundaries, baslik_cleaned, konu_sayisi_int)
            if first_match is not None:
                validation_result, konu_listesi = ex_ob_tablosu_konu_sinirli_arama(
                    boundaries['content'], first_match, baslik_cleaned, konu_sayisi_int, all_matched_headers
                )
                
                formatted_content_parts.append(
                    f"{i}-{baslik} ({konu_sayisi_int}) -> 1. EÅŸleÅŸme\\n"
                    f"{validation_result}\\n"
                )
                
                structured_konu_data.append({
                    'ogrenme_birimi': baslik,
                    'sira': i,
                    'konu_sayisi': konu_sayisi_int,
                    'konular': konu_listesi
                })
    
    return header_match_info, formatted_content_parts, structured_konu_data

# CONSOLIDATED: Merged with find_header_matches_with_validation logic
def _validate_single_header_match(boundaries, baslik_cleaned, konu_sayisi_int, return_first_only=False):
    """Unified header validation - can return first match or all matches"""
    content_processor = boundaries['processor']
    matches = find_baslik_matches(baslik_cleaned, content_processor)
    valid_matches = []
    
    for match_idx in matches:
        after_baslik_pos = match_idx + len(baslik_cleaned)
        after_baslik = boundaries['content'][after_baslik_pos:]
        
        if konu_sayisi_int > 0:
            found_numbers = validate_konu_patterns(after_baslik, konu_sayisi_int)
            if found_numbers == konu_sayisi_int:
                if return_first_only:
                    return match_idx
                valid_matches.append(match_idx)
    
    return None if return_first_only else valid_matches

def find_first_valid_match(boundaries, baslik_cleaned, konu_sayisi_int):
    """Find the first valid match position for a header"""
    return _validate_single_header_match(boundaries, baslik_cleaned, konu_sayisi_int, return_first_only=True)

# DBF PDF'den temel ders bilgilerini Ã§Ä±karÄ±r - ders adÄ±, sÄ±nÄ±f, sÃ¼re, amaÃ§ gibi bilgileri parse eder
# Pattern matching ile "DERSÄ°N ADI:", "DERSÄ°N SINIFI:" gibi alanlarÄ± bulur ve iÃ§eriklerini Ã§Ä±karÄ±r
def ex_temel_bilgiler(text):
    """
    extract_olcme.py'den kopyalandi - DBF'den temel ders bilgilerini cikarir
    
    Args:
        text (str): PDF/DOCX'den Ã§Ä±karÄ±lan tam metin
        
    Returns:
        dict: Temel ders bilgileri
    """
    # Varyasyonlarla case-sensitive yapÄ±
    patterns = [
        (["DERSÄ°N ADI", "ADI"], ["DERSÄ°N", "DERSÄ N"]),                                  # Dersin AdÄ±
        (["DERSÄ°N SINIFI", "SINIFI"], ["DERSÄ°N", "DERSÄ N"]),                            # Sinifi (metin olarak)!!
        (["DERSÄ°N SÃœRESÄ°", "SÃœRESÄ°"], ["DERSÄ°N", "DERSÄ N"]),                            # SÃ¼re/Ders saati (metin olarak)!!
        (["DERSÄ°N AMACI", "AMACI"], ["DERSÄ°N", "DERSÄ N"]),                              # Dersin AmacÄ±
        (["DERSÄ°N KAZANIMLARI", "KAZANIMLARI"], ["EÄÄ°TÄ°M", "EÄÄ TÄ M", "EÄ", "DONAT"]),   # KazanÄ±m -> Madde yapÄ±lmalÄ±
        (["DONANIMI"], ["Ã–LÃ‡", "DEÄERLENDÄ°RME"]),                                       # Ortam/DonanÄ±m
        (["DEÄERLENDÄ°RME"], ["DERSÄ°N", "DERSÄ N", "KAZANIM", "Ã–ÄRENME"]),                # Ã–lÃ§me-DeÄŸerlendirme
    ]
    result = {}
    
    # Normalize edilmiÅŸ metin sadece eÅŸleÅŸtirme iÃ§in
    text_normalized = normalize_turkish_text(text)

    for i, (start_keys, end_keys) in enumerate(patterns, 1):
        start_index = None
        start_match = ""
        start_original_idx = None
        
        for sk in start_keys:
            sk_normalized = normalize_turkish_text(sk)
            idx = text_normalized.find(sk_normalized)
            if idx != -1 and (start_index is None or idx < start_index):
                start_index = idx + len(sk_normalized)
                start_match = sk
                # Orijinal metinde karÅŸÄ±lÄ±k gelen pozisyonu bul
                start_original_idx = text.upper().find(sk.upper())
                if start_original_idx != -1:
                    start_original_idx += len(sk)
        
        if start_index is None or start_original_idx is None:
            continue
            
        end_index = None
        end_original_idx = None
        
        for ek in end_keys:
            ek_normalized = normalize_turkish_text(ek)
            idx = text_normalized.find(ek_normalized, start_index)
            if idx != -1 and (end_index is None or idx < end_index):
                end_index = idx
                # Orijinal metinde karÅŸÄ±lÄ±k gelen pozisyonu bul
                end_original_idx = text.upper().find(ek.upper(), start_original_idx)
                
        if end_original_idx is not None:
            section = text[start_original_idx:end_original_idx].strip()
        else:
            section = text[start_original_idx:].strip()
            
        section = re.sub(r'\s+', ' ', section)
        result[f"Case{i}_{start_match}"] = section
    return result

# KazanÄ±m sayÄ±sÄ± ve sÃ¼re tablosunu parse eder - her Ã¶ÄŸrenme birimi iÃ§in kazanÄ±m sayÄ±sÄ± ve ders saati bilgilerini Ã§Ä±karÄ±r
# "KAZANIM SAYISI VE SÃœRE TABLOSU" baÅŸlÄ±ÄŸÄ±nÄ± bulur ve altÄ±ndaki tabloyu structured data formatÄ±na Ã§evirir
def ex_kazanim_tablosu(full_text):
    """full_text'ten KAZANIM SAYISI VE SÃœRE TABLOSU'nu Ã§Ä±karÄ±r ve formatlÄ± string ile yapÄ±landÄ±rÄ±lmÄ±ÅŸ veri dÃ¶ndÃ¼rÃ¼r"""
    try:

        table_start_patterns = [
            "KAZANIM SAYISI VE SÃœRE TABLOSU", 
            "DERSÄ°N KAZANIM TABLOSU", 
            "TABLOSU",
            "TABLOS U", 
            "TABLO SU", 
            "TABL OSU", 
            "TAB LOSU", 
            "TA BLOSU"
        ]
        
        # TÃ¼rkÃ§e karakterleri normalize ederek ara
        full_text_normalized = normalize_turkish_text(full_text)
        
        earliest_start = None
        earliest_idx = len(full_text_normalized)
        table_start = None
        for pattern in table_start_patterns:
            pattern_normalized = normalize_turkish_text(pattern)
            idx = full_text_normalized.find(pattern_normalized)
            if idx != -1 and idx < earliest_idx:
                earliest_idx = idx
                earliest_start = idx + len(pattern_normalized)
                table_start = pattern
        
        start_idx = earliest_start
        if table_start is None:
            return "KAZANIM SAYISI VE SÃœRE TABLOSU bulunamadÄ±", []

        end_markers = ["TOPLAM", "Ã–ÄRENME BÄ°RÄ°MÄ°"]
        end_idx = len(full_text_normalized)
        for marker in end_markers:
            marker_normalized = normalize_turkish_text(marker)
            idx = full_text_normalized.find(marker_normalized, start_idx + 50)
            if idx != -1 and idx < end_idx:
                end_idx = idx
        
        # Orijinal metinden section al ama pozisyonlarÄ± normalize edilmiÅŸ metinden bul
        table_section_original = full_text[start_idx:end_idx].strip()
        table_section_normalized = full_text_normalized[start_idx:end_idx].strip()

        toplam_normalized = normalize_turkish_text("TOPLAM")
        if toplam_normalized in table_section_normalized:
            toplam_idx = table_section_normalized.find(toplam_normalized)
            after_toplam = table_section_normalized[toplam_idx:]
            toplam_end = re.search(r'TOPLAM.*?100', after_toplam, re.IGNORECASE)
            if toplam_end:
                table_section_original = table_section_original[:toplam_idx + toplam_end.end()]
                table_section_normalized = table_section_normalized[:toplam_idx + toplam_end.end()]

        # BaÅŸlÄ±k satÄ±rÄ±nÄ± kaldÄ±r (Ã–ÄRENME BÄ°RÄ°MÄ° KAZANIM SAYISI DERS SAATÄ° ORAN) - Normalize edilmiÅŸ karakterlerle
        header_patterns = [
            # NORMALIZE EDÄ°LMÄ°Å KARAKTERLER - table_section_normalized iÃ§in
            r'OGRENME.*?\(\s*%\s*\)',
            r'OGRENME.*?\(%\)',
            r'OGRENME.*?ORAN.*?\(\s*%\s*\)',
            r'OGRENME.*?ORAN.*?\(%\)',
            r'KAZANIM(?:.|\\n)*?ORAN\s*\(\s*%\s*\)',  # geniÅŸ eÅŸleÅŸme, tÃ¼m baÅŸlÄ±k bloÄŸunu kaldÄ±rÄ±r
            r'KAZANIM SAYISI VE\s*SURE TABLOSU\s*OGRENME BIRIMI\s*KAZANIM\s*SAYISI\s*DERS SAATI\s*ORAN\s*\(\s*%\s*\)',  # tam uyumlu eÅŸleÅŸme
            r'OGRENME(?:.|\\n)*?ORAN(?:.|\\n)*?\(\s*%\s*\)'  # geniÅŸ pattern
        ]
        
        for header_pattern in header_patterns:
            new_table_section_normalized = re.sub(header_pattern, '', table_section_normalized, flags=re.IGNORECASE | re.DOTALL).strip()
            if len(new_table_section_normalized) < len(table_section_normalized):
                table_section_normalized = new_table_section_normalized
                # AynÄ± pattern'i orijinal metne de uygula (TÃ¼rkÃ§e karakterlerle)
                original_pattern = header_pattern.replace('OGRENME', 'Ã–ÄRENME').replace('SURE', 'SÃœRE').replace('BIRIMI', 'BÄ°RÄ°MÄ°').replace('SAATI', 'SAATÄ°')
                table_section_original = re.sub(original_pattern, '', table_section_original, flags=re.IGNORECASE | re.DOTALL).strip()
                break
        
        # Ã‡Ä±ktÄ± iÃ§in orijinal metni kullan (header kaldÄ±rÄ±lmÄ±ÅŸ hali)
        table_section = table_section_original

        lines = []
        structured_data = []
        
        # Her satÄ±r: Ä°sim + sayÄ±lar + % pattern'i - Git geÃ§miÅŸinden geliÅŸmiÅŸ versiyonlar
        patterns = [
            r'([^0-9]+?)\s+(\d+)\s+(\d+)\s*/\s*(\d+)\s+(\d+(?:[,\.]\d+)?)',  # Kesirli format + oran (18/36)
            r'([^0-9]+?)\s+(\d+)\s+(\d+)\s+(\d+(?:[,\.]\d+)?)',              # Normal format + oran 
            r'([^0-9]+?)\s+(\d+)\s+(\d+)(?:\s|$)',                           # Sadece 2 sÃ¼tun (oran yok)
            r'([^0-9]+?)\s+(\d+)\s+(\d+(?:[,\.]\d+)?)\s*%?',                 # YÃ¼zde iÅŸareti opsiyonel
            r'([^0-9]+?)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\d+(?:[,\.]\d+)?)',      # 5 sÃ¼tun format
            r'([^0-9]+?)\s+(\d+)\s+(\d+(?:[,\.]\d+)?)',                      # Basit 2 sÃ¼tun format
        ]

        matches = []
        for pattern in patterns:
            # Normalize edilmiÅŸ metinde pattern ara ama orijinal metinden deÄŸerleri al
            normalized_matches = re.findall(pattern, table_section_normalized)
            if normalized_matches:
                # Orijinal metinde de aynÄ± pattern'i ara
                original_matches = re.findall(pattern, table_section_original)
                if original_matches:
                    matches = original_matches
                    break
        
        for match in matches:
            ogrenme_birimi = re.sub(r'\s+', ' ', match[0].strip()).strip()
            kazanim_sayisi = match[1]
            
            # GeliÅŸmiÅŸ pattern matching: 2-6 grup desteÄŸi
            if len(match) == 2:
                # Sadece isim + kazanÄ±m sayÄ±sÄ±
                ders_saati = "-"
                oran = "-"
            elif len(match) == 3:
                # Ä°sim + kazanÄ±m + ders saati/oran
                ders_saati = match[2]
                oran = "-"
                # EÄŸer 3. alan % iÃ§eriyorsa oran olabilir
                if '%' in str(match[2]) or ',' in str(match[2]) or '.' in str(match[2]):
                    oran = match[2]
                    ders_saati = "-"
            elif len(match) == 4:
                # Ä°sim + kazanÄ±m + ders saati + oran
                ders_saati = match[2]
                oran = match[3]
            elif len(match) == 5:
                # Ä°sim + kazanÄ±m + ders saati + kesir + oran veya 5 sÃ¼tun format
                if '/' in str(match[3]):
                    # Kesirli format: isim + kazanÄ±m + ders saati + kesir + oran
                    ders_saati = f"{match[2]}/{match[3]}"
                    oran = match[4]
                else:
                    # 5 sÃ¼tun format: isim + kazanÄ±m + sÃ¼tun3 + ders saati + oran
                    ders_saati = match[3]
                    oran = match[4]
            elif len(match) == 6:
                # Tam format: isim + kazanÄ±m + sÃ¼tun3 + sÃ¼tun4 + ders saati + oran
                ders_saati = match[4]
                oran = match[5]
            
            # Use cached normalization instead of calling twice
            if ogrenme_birimi.upper().strip() != "TOPLAM":
                lines.append(f"{ogrenme_birimi}, {kazanim_sayisi}, {ders_saati}, {oran}")
                structured_data.append({
                    'title': ogrenme_birimi,
                    'count': int(kazanim_sayisi),
                    'duration': ders_saati,
                    'percentage': oran
                })
        
        if lines:
            result = "KAZANIM SAYISI VE SÃœRE TABLOSU:\\n"
            for idx, line in enumerate(lines, 1):
                result += f"{idx}-{line}\\n"
            return result.strip(), structured_data
        else:
            return "KAZANIM SAYISI VE SÃœRE TABLOSU - Veri bulunamadÄ±", []
                
    except Exception as e:
        return f"KAZANIM SAYISI VE SÃœRE TABLOSU - HATA: {str(e)}", []

# PDF'den Ã¶ÄŸrenme birimi alanÄ±nÄ± Ã§Ä±karÄ±r - en karmaÅŸÄ±k ve ana fonksiyon
# KazanÄ±m tablosundaki baÅŸlÄ±klarÄ± metinde bulur, konu listelerini Ã§Ä±karÄ±r ve structured data formatÄ±nda dÃ¶ndÃ¼rÃ¼r
def ex_ob_tablosu(full_text):
    """
    REFACTORED - PDF'den Ã–ÄŸrenme Birimi AlanÄ±nÄ± Ã§Ä±karÄ±r
    
    Args:
        full_text (str): PDF/DOCX'den Ã§Ä±karÄ±lan tam metin
        
    Returns:
        tuple: (Ã¶ÄŸrenme_birimi_analiz_sonucu_string, structured_konu_data)
    """
    try:
        # Extract table boundaries using centralized logic
        boundaries = extract_table_boundaries(full_text)
        if boundaries is None:
            return "Ã–ÄŸrenme Birimi AlanÄ± - BaÅŸlangÄ±Ã§ kelimeleri bulunamadÄ±", []
        
        # Get kazanim data
        kazanim_tablosu_str, kazanim_tablosu_data = ex_kazanim_tablosu(full_text)
        
        if not kazanim_tablosu_data:
            # Return raw content if no structured data available
            content = boundaries['content']
            if len(content) <= 400:
                formatted_content = content
            else:
                formatted_content = f"{content[:200]}\\n...\\n{content[-200:]}"
            
            result = f"{'--'*25}\\nÃ–ÄŸrenme Birimi AlanÄ±:\\n{'--'*25}\\n{formatted_content}"
            return result, []
        
        # Adjust table start if first title is found
        if kazanim_tablosu_data:
            first_title = kazanim_tablosu_data[0]['title']
            processor = TextProcessor(full_text)
            first_title_idx = processor.find_normalized(first_title, boundaries['start_idx'])
            if first_title_idx != -1:
                # Recalculate boundaries with new start position
                new_content = full_text[first_title_idx:boundaries['end_idx']].strip()
                new_content = re.sub(r'\\s+', ' ', new_content).strip()
                boundaries['content'] = new_content
                boundaries['processor'] = TextProcessor(new_content)
                boundaries['start_idx'] = first_title_idx
        
        # Find and validate header matches
        all_matched_headers = find_header_matches_with_validation(boundaries, kazanim_tablosu_data)
        
        # Process headers and extract content
        header_match_info, formatted_content_parts, structured_konu_data = process_header_matches(
            boundaries, kazanim_tablosu_data, all_matched_headers
        )
        
        # Format final result
        if formatted_content_parts:
            formatted_content = "\\n".join(formatted_content_parts)
        else:
            content = boundaries['content']
            if len(content) <= 400:
                formatted_content = content
            else:
                formatted_content = f"{content[:200]}\\n...\\n{content[-200:]}"
        
        result = f"{'--'*25}\\nÃ–ÄŸrenme Birimi AlanÄ±:{header_match_info}{'--'*25}\\n{formatted_content}"
        return result, structured_konu_data
        
    except Exception as e:
        return f"Hata: {str(e)}", []

# Konu iÃ§eriÄŸinden kazanÄ±m listesini Ã§Ä±karÄ±r - "1.1.", "â€¢", "a)" gibi pattern'leri tespit eder
# Her konu metnindeki alt maddeleri bulur ve structured data formatÄ±nda kazanÄ±m listesi dÃ¶ndÃ¼rÃ¼r
def extract_kazanimlar_from_konu_content(konu_content, text_processor=None):
    """
    OPTIMIZED - Konu iÃ§eriÄŸinden kazanÄ±mlarÄ± Ã§Ä±karÄ±r
    """
    import re
    
    if not konu_content or len(konu_content.strip()) < 10:
        return []
    
    # Use cached text processor if available
    if text_processor is None:
        text_processor = TextProcessor(konu_content)
    
    kazanimlar = []
    
    # Consolidated kazanÄ±m patterns
    patterns = [
        r'(\d+\.\d+\.)\s*(.+?)(?=\n\d+\.\d+\.|\n[A-Z]|\nâ€¢|\n-|\n[a-z]\)|\n\d+\.|\Z)',
        r'([â€¢-])\s*(.+?)(?=\n[â€¢-]|\n\d+\.|\n[A-Z]|\n[a-z]\)|\Z)',
        r'([a-z]\))\s*(.+?)(?=\n[a-z]\)|\n\d+\.|\n[A-Z]|\n[â€¢-]|\Z)',
        r'(\(\d+\))\s*(.+?)(?=\n\(\d+\)|\n\d+\.|\n[A-Z]|\n[â€¢-]|\n[a-z]\)|\Z)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text_processor.original, re.MULTILINE | re.DOTALL)
        if matches:
            sira = 1
            for marker, text in matches:
                cleaned_text = text_processor.normalize_text(text)
                
                if 5 <= len(cleaned_text) <= 200 and _is_valid_kazanim(cleaned_text):
                    kazanimlar.append({
                        'kazanim_adi': cleaned_text[:197] + "..." if len(cleaned_text) > 200 else cleaned_text,
                        'sira': sira,
                        'marker': marker.strip()
                    })
                    sira += 1
            
            if kazanimlar:
                break
    
    return kazanimlar

# KazanÄ±m metninin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder - geÃ§ersiz kelimeler varsa reddeder
# "DERS", "HAFTA", "SAAT" gibi baÅŸlÄ±k kelimelerini iÃ§eren metinleri kazanÄ±m olarak kabul etmez
def _is_valid_kazanim(text):
    """
    OPTIMIZED - Simplified kazanim validation
    """
    if not text or len(text) < 5:
        return False
    
    # Quick check for invalid patterns
    text_upper = text.upper()
    invalid_keywords = ['DERS ', 'HAFTA', 'SAAT', 'TOPLAM', 'UYGULAMA', 'FAALÄ°YET', 
                       'TEMRÄ°N', 'DEÄERLEND', 'SINAMA', 'Ã–LÃ‡ME', 'TEST', 'Ã–RNEK']
    
    return not any(keyword in text_upper for keyword in invalid_keywords) and \
           (len(text) >= 10 or any(c.isalpha() for c in text))

# Ã‡alÄ±ÅŸma alanÄ± sÄ±nÄ±rlarÄ±nÄ± belirler - baÅŸlÄ±ktan sonra hangi kÄ±sÄ±m iÅŸlenecek
# DiÄŸer baÅŸlÄ±klarÄ± ve stop kelimeleri kullanarak alan sÄ±nÄ±rlarÄ±nÄ± Ã§izer
def _find_work_area_boundaries(after_baslik, all_matched_headers, baslik_idx, baslik):
    """OPTIMIZED - Helper: Find work area boundaries for content extraction"""
    import re
    
    next_pos = len(after_baslik)
    
    # Check matched headers
    if all_matched_headers:
        current_pos_in_text = baslik_idx + len(baslik)
        for header_info in all_matched_headers:
            other_pos = header_info.get('position', -1)
            if other_pos > current_pos_in_text:
                relative_pos = other_pos - current_pos_in_text
                if relative_pos < next_pos:
                    next_pos = relative_pos
    
    # Check general patterns
    if next_pos == len(after_baslik):
        patterns = [r'\\n[A-ZÃœÄIÅÃ–Ã‡][A-ZÃœÄIÅÃ–Ã‡\\s]{10,}', r'\\n\\d+\\.\\s*[A-ZÃœÄIÅÃ–Ã‡]', 
                   r'DERSÄ°N|DERSÄ N', r'UYGULAMA|FAALÄ°YET|TEMRÄ°N']
        
        for pattern in patterns:
            match = re.search(pattern, after_baslik)
            if match and match.start() < next_pos:
                next_pos = match.start()
    
    return after_baslik[:next_pos]

# Belirli bir konu numarasÄ± iÃ§in iÃ§erik Ã§Ä±karÄ±r - "1. ", "2. " pattern'leri ile sÄ±nÄ±rlar bulur
# Konu baÅŸlÄ±ÄŸÄ± ve iÃ§eriÄŸini ayÄ±rarak temizlenmiÅŸ metin dÃ¶ndÃ¼rÃ¼r
def _extract_konu_content(work_area, konu_no, konu_sayisi, current_pos):
    """OPTIMIZED - Helper: Extract content for a specific konu number"""
    patterns = [f"{konu_no}. ", f"{konu_no} "]
    
    for pattern in patterns:
        pos = work_area.find(pattern, current_pos)
        if pos != -1 and is_valid_madde_baslik(work_area, pos, konu_no):
            # Find end position
            end_pos = len(work_area)
            if konu_no < konu_sayisi:
                next_patterns = [f"{konu_no + 1}. ", f"{konu_no + 1} "]
                for next_pattern in next_patterns:
                    next_pos = work_area.find(next_pattern, pos + 1)
                    if next_pos != -1 and is_valid_madde_baslik(work_area, next_pos, konu_no + 1):
                        end_pos = next_pos
                        break
            
            content = work_area[pos:end_pos].strip()
            # Clean the number prefix
            for p in patterns:
                if content.startswith(p):
                    content = content[len(p):].strip()
                    break
            
            return content, pos + 1
    
    return None, current_pos + 1

# BaÅŸlÄ±k eÅŸleÅŸmesinden sonra konu yapÄ±sÄ±nÄ± sÄ±ralÄ± rakamlarla parse eder ve kazanÄ±mlarÄ± Ã§Ä±karÄ±r
# "1. Konu AdÄ±", "2. DiÄŸer Konu" ÅŸeklindeki yapÄ±yÄ± tespit eder ve her konu iÃ§in kazanÄ±m listesi oluÅŸturur
def ex_ob_tablosu_konu_sinirli_arama(text, baslik_idx, baslik, konu_sayisi, all_matched_headers=None):
    """
    BaÅŸlÄ±k eÅŸleÅŸmesinden sonra konu yapÄ±sÄ±nÄ± sÄ±ralÄ± rakamlarla doÄŸrular - 2 dÃ¶ngÃ¼
    
    â­ YENÄ°: ArtÄ±k her konu iÃ§in kazanÄ±mlarÄ± da Ã§Ä±karÄ±r
    
    Returns:
        tuple: (formatted_text, structured_konu_data_with_kazanimlar)
    """
    import re
    
    # BaÅŸlÄ±k eÅŸleÅŸmesinden sonraki tÃ¼m metni al
    after_baslik = text[baslik_idx + len(baslik):]
    
    # CONSOLIDATED: Use helper function instead of duplicate code
    work_area = _find_work_area_boundaries(after_baslik, all_matched_headers, baslik_idx, baslik)
    validation_info = []  # String Ã§Ä±ktÄ± iÃ§in (backward compatibility)
    structured_konu_data = []  # â­ YENÄ°: Structured data iÃ§in
    
    # CONSOLIDATED: Use optimized helper function with cached text processor
    text_processor = TextProcessor(work_area)  # Cache normalization
    current_pos = 0
    
    for konu_no in range(1, konu_sayisi + 1):
        content, current_pos = _extract_konu_content(work_area, konu_no, konu_sayisi, current_pos)
        
        if content:
            lines = content.split('\n')
            konu_adi = lines[0].strip() if lines else content[:50].strip()
            
            # Extract kazanimlar with cached text processor
            kazanimlar = extract_kazanimlar_from_konu_content(content, text_processor)
            
            structured_konu_data.append({
                'konu_adi': konu_adi,
                'sira': konu_no,
                'content': content,
                'kazanimlar': kazanimlar
            })
            
            kazanim_preview = f" ({len(kazanimlar)} kazanÄ±m)" if kazanimlar else ""
            validation_info.append(f"{konu_no}. {konu_adi}{kazanim_preview}")
    
    return "\\n".join(validation_info), structured_konu_data

# Yedek plan fonksiyonu - normal baÅŸlÄ±k eÅŸleÅŸtirme baÅŸarÄ±sÄ±z olduÄŸunda alternatif yÃ¶ntem dener
# "1" rakamÄ±nÄ± arayarak potansiyel baÅŸlÄ±k pozisyonlarÄ±nÄ± bulur ve doÄŸrular
def ex_ob_tablosu_konu_bulma_yedek_plan(text, original_baslik, konu_sayisi):
    """Son eÅŸleÅŸen baÅŸlÄ±ktan sonra '1' rakamÄ±nÄ± bulup alternatif eÅŸleÅŸme arar"""
    import re
    
    # "1" rakamÄ±nÄ± ara - daha basit pattern
    one_pattern = r'1\\.'
    matches = list(re.finditer(one_pattern, text))
    
    if not matches:
        return None
    
    # Her "1" pozisyonu iÃ§in kontrol et
    for match in matches:
        one_pos = match.start()
        
        # "1" den Ã¶nceki cÃ¼mleyi bul (maksimum 100 karakter geriye git)
        start_search = max(0, one_pos - 100)
        before_one = text[start_search:one_pos]
        
        # CÃ¼mle baÅŸlangÄ±cÄ±nÄ± bul (bÃ¼yÃ¼k harf ile baÅŸlayan kelimeler)
        sentences = re.split(r'[.!?]', before_one)
        if sentences:
            potential_title = sentences[-1].strip()
            
            # Potansiyel baÅŸlÄ±k Ã§ok kÄ±saysa atla
            if len(potential_title) < 10:
                continue
                
            # "1" den sonra konu sayÄ±sÄ± kadar rakamÄ± kontrol et
            after_one = text[one_pos:]
            found_numbers = 0
            for rakam in range(1, konu_sayisi + 1):
                patterns = [f"{rakam}. ", f"{rakam} "]
                pattern_found = False
                for pattern in patterns:
                    pos = after_one[:500].find(pattern)  # Ä°lk 500 karakterde ara
                    if pos != -1:
                        # Context-aware validation: gerÃ§ek madde baÅŸlÄ±ÄŸÄ± mÄ±?
                        if is_valid_madde_baslik(after_one[:500], pos, rakam):
                            pattern_found = True
                            break
                if pattern_found:
                    found_numbers += 1
            
            # TÃ¼m rakamlar bulunduysa alternatif eÅŸleÅŸme geÃ§erli
            if found_numbers == konu_sayisi:
                return {
                    'title': potential_title,
                    'position': one_pos,
                    'numbers_found': found_numbers
                }
    
    return None

# DBF klasÃ¶rÃ¼ndeki tÃ¼m PDF/DOCX dosyalarÄ±nÄ± tarar ve geÃ§erli dosyalarÄ± listeler
# Dosya bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ kontrolÃ¼ yapar ve sadece okuttirilebilir dosyalarÄ± dÃ¶ndÃ¼rÃ¼r
def get_all_dbf_files(validate_files=True):
    """
    DBF PDF ve DOCX dosyalarÄ±nÄ± bulma ve yÃ¶netme fonksiyonu - API sistemine optimize edildi
    
    Args:
        validate_files (bool): Dosya bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ kontrolÃ¼ yap (varsayÄ±lan: True)
    
    Returns:
        list: PDF ve DOCX dosya yollarÄ± listesi (sadece geÃ§erli dosyalar)
    """
    import os
    
    # utils_env modÃ¼lÃ¼nÃ¼ kullan
    try:
        from .utils_env import get_data_path, get_project_root
    except ImportError:
        from modules.utils_env import get_data_path, get_project_root
    
    project_root = get_project_root()
    base_path = get_data_path("dbf")
    
    # Debug: base_path'i konsola yazdÄ±r
    print(f"ğŸ“ PROJECT_ROOT: {project_root}")
    print(f"ğŸ“ DBF tarama yolu: {base_path}")
    print(f"ğŸ“ KlasÃ¶r mevcut mu: {os.path.exists(base_path)}")
    
    def is_valid_document(file_path):
        """DosyanÄ±n geÃ§erli bir PDF veya DOCX dosyasÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        if not validate_files:
            return True
            
        try:
            # PyMuPDF ile dosyayÄ± aÃ§mayÄ± dene
            doc = fitz.open(file_path)
            
            # Dosya aÃ§Ä±labildi, temel kontroller yap
            page_count = len(doc)
            if page_count == 0:
                doc.close()
                return False
            
            # Ä°lk sayfayÄ± okumayÄ± dene
            page = doc.load_page(0)
            text = page.get_text()
            doc.close()
            
            # EÄŸer hiÃ§ metin yoksa ve sayfa sayÄ±sÄ± 1'den azsa bozuk olabilir
            if not text.strip() and page_count <= 1:
                return False
                
            return True
            
        except Exception as e:
            # Dosya aÃ§Ä±lamÄ±yorsa veya hata varsa geÃ§ersiz
            print(f"âš ï¸  Bozuk dosya atlandÄ±: {os.path.basename(file_path)} - {str(e)}")
            return False
    
    # TÃ¼m PDF ve DOCX dosyalarÄ±nÄ± bul ve validate et
    all_files = []
    supported_extensions = ('.pdf', '.docx')
    skipped_files = 0
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower().endswith(supported_extensions):
                file_path = os.path.join(root, file)
                
                # Dosya bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ kontrolÃ¼
                if is_valid_document(file_path):
                    all_files.append(file_path)
                else:
                    skipped_files += 1
    
    # SonuÃ§ bilgilerini yazdÄ±r
    print(f"ğŸ“Š Toplam {len(all_files)} geÃ§erli dosya bulundu")
    if validate_files and skipped_files > 0:
        print(f"ğŸ“Š Toplam {skipped_files} bozuk dosya iÅŸleme alÄ±nmadÄ±.")
    
    return all_files

def read_full_text_from_file(file_path):
    """
    PDF veya DOCX dosyasÄ±ndan tam metni okur (PyMuPDF ile unified processing)
    
    Args:
        file_path (str): Dosya yolu
        
    Returns:
        str: Dosyadan Ã§Ä±karÄ±lan tam metin
    """
    try:
        doc = fitz.open(file_path)
        full_text = ""
        for page in doc:
            full_text += page.get_text() + "\\n"
        doc.close()
        
        # Metni normalize et
        full_text = re.sub(r'\s+', ' ', full_text)
        return full_text
        
    except Exception as e:
        print(f"Error reading file {file_path}: {str(e)}")
        return ""

# TÃ¼rkÃ§e karakter normalizasyon fonksiyonu - Ã§Ä±kÄ±ÅŸ, Ã¶ÄŸrenme gibi kelimelerdeki Ã¶zel karakterleri ASCII'ye Ã§evirir
# Case-insensitive eÅŸleÅŸtirme iÃ§in tÃ¼m karakterleri bÃ¼yÃ¼k harfe Ã§evirir ve whitespace normalize eder
def normalize_turkish_text(text):
    """TÃ¼rkÃ§e karakterleri normalize eder ve case-insensitive karÅŸÄ±laÅŸtÄ±rma iÃ§in hazÄ±rlar"""
    if not text:
        return ""
    
    # TÃ¼rkÃ§e karakterleri ASCII'ye dÃ¶nÃ¼ÅŸtÃ¼r ve bÃ¼yÃ¼k harfe Ã§evir
    # Ä° -> I, i -> i, ÄŸ -> g, Ã¼ -> u, ÅŸ -> s, Ã¶ -> o, Ã§ -> c
    char_map = {
        'Ä°': 'I', 
        'Ä±': 'i', 
        'Ä': 'G', 
        'ÄŸ': 'g',
        'Ãœ': 'U', 
        'Ã¼': 'u', 
        'Å': 'S', 
        'ÅŸ': 's', 
        'Ã–': 'O', 
        'Ã¶': 'o', 
        'Ã‡': 'C', 
        'Ã§': 'c'
    }
    
    # Karakterleri deÄŸiÅŸtir
    normalized = text
    for turkish_char, ascii_char in char_map.items():
        normalized = normalized.replace(turkish_char, ascii_char)
    
    # BÃ¼yÃ¼k harfe Ã§evir ve whitespace normalize et
    normalized = re.sub(r'\s+', ' ', normalized.upper().strip())
    
    return normalized

# Madde baÅŸlÄ±ÄŸÄ± doÄŸrulama fonksiyonu - "15-20. yÃ¼zyÄ±l" gibi tarih ifadelerini madde numarasÄ± olarak algÄ±lamaz
# Context-aware validation ile gerÃ§ek "1. ", "2. " madde baÅŸlÄ±klarÄ±nÄ± tespit eder
def is_valid_madde_baslik(text, pos, rakam):
    """
    Bulunan rakam pattern'inin gerÃ§ek madde numarasÄ± mÄ± yoksa tarih/yÃ¼zyÄ±l mÄ± olduÄŸunu kontrol eder.
    
    Args:
        text (str): Aranacak metin
        pos (int): Bulunan pattern pozisyonu
        rakam (int): Aranan rakam (1, 2, 3...)
        
    Returns:
        bool: True = gerÃ§ek madde baÅŸlÄ±ÄŸÄ±, False = tarih/yÃ¼zyÄ±l
    """
    import re
    
    # Pattern'den sonraki metni al
    pattern_len = len(f"{rakam}. ")
    after_pattern = text[pos + pattern_len:] if pos + pattern_len < len(text) else ""
    
    if not after_pattern.strip():
        return False
    
    # Ä°lk kelimeyi bul
    words = after_pattern.strip().split()
    if not words:
        return False
        
    first_word = words[0].lower()
    
    # Zaman belirten kelimeler listesi
    time_words = [
        "yÃ¼zyÄ±l", "yÃ¼zyÄ±lda", "yÃ¼zyÄ±ldan", "yÃ¼zyÄ±lÄ±n", "yÃ¼zyÄ±la", 
        "asÄ±r", "asÄ±rda", "asÄ±rdan", "asÄ±rÄ±n", "asÄ±ra",
        "dÃ¶nem", "dÃ¶nemde", "dÃ¶nemden", "dÃ¶nemin", "dÃ¶neme",
        "yÄ±l", "yÄ±lda", "yÄ±ldan", "yÄ±lÄ±n", "yÄ±la",
        "sene", "senede", "seneden", "senenin", "seneye"
    ]
    
    # EÄŸer ilk kelime zaman belirtiyorsa madde baÅŸlÄ±ÄŸÄ± deÄŸil
    if first_word in time_words:
        return False
    
    # SatÄ±r baÅŸÄ±nda mÄ± kontrol et (madde baÅŸlÄ±klarÄ± genelde satÄ±r baÅŸÄ±nda olur)
    if pos > 0:
        char_before = text[pos - 1]
        # Ã–ncesinde yeni satÄ±r, boÅŸluk veya tab olmalÄ±
        if char_before not in ['\n', '\r', ' ', '\t']:
            return False
    
    # Ä°lk kelime bÃ¼yÃ¼k harfle baÅŸlÄ±yor mu? (madde baÅŸlÄ±klarÄ± bÃ¼yÃ¼k harfle baÅŸlar)
    if not (words[0] and words[0][0].isupper()):
        return False
        
    return True

# ===========================
# MAIN PROCESSING FUNCTIONS  
# ===========================

# Tek DBF dosyasÄ±nÄ± tam olarak iÅŸler - temel bilgiler, kazanÄ±m tablosu ve Ã¶ÄŸrenme birimi analizi yapar
# API endpoint'leri tarafÄ±ndan Ã§aÄŸrÄ±lan ana processing fonksiyonu
def process_dbf_file(file_path):
    """
    Tek DBF dosyasÄ±nÄ± iÅŸler ve sonuÃ§larÄ± dÃ¶ndÃ¼rÃ¼r
    
    Args:
        file_path (str): Ä°ÅŸlenecek dosya yolu
        
    Returns:
        dict: Ä°ÅŸlem sonucu
    """
    try:
        # Dosyadan tam metni oku
        full_text = read_full_text_from_file(file_path)
        
        if not full_text.strip():
            return {"success": False, "error": "Dosya iÃ§eriÄŸi boÅŸ", "file_path": file_path}
        
        # Temel bilgileri Ã§Ä±kar
        temel_bilgiler = ex_temel_bilgiler(full_text)
        
        # KazanÄ±m tablosunu Ã§Ä±kar
        kazanim_tablosu_str, kazanim_tablosu_data = ex_kazanim_tablosu(full_text)
        
        # Ã–ÄŸrenme birimi analizini yap
        ob_analiz = ex_ob_tablosu(full_text)
        
        return {
            "success": True,
            "file_path": file_path,
            "filename": os.path.basename(file_path),
            "temel_bilgiler": temel_bilgiler,
            "kazanim_tablosu_data": kazanim_tablosu_data,
            "ogrenme_birimi_analizi": ob_analiz
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "file_path": file_path,
            "filename": os.path.basename(file_path)
        }

# Ã‡oklu DBF dosyasÄ± iÅŸleme fonksiyonu - dosya listesini alÄ±r ve her birini sÄ±rasÄ±yla iÅŸler
# BaÅŸarÄ± ve hata istatistiklerini tutar, toplu sonuÃ§ raporu Ã¼retir
def process_multiple_dbf_files(file_paths):
    """
    Birden fazla DBF dosyasÄ±nÄ± iÅŸler
    
    Args:
        file_paths (list): Ä°ÅŸlenecek dosya yollarÄ±
        
    Returns:
        dict: Toplu iÅŸlem sonucu
    """
    results = []
    success_count = 0
    error_count = 0
    
    for file_path in file_paths:
        result = process_dbf_file(file_path)
        results.append(result)
        
        if result["success"]:
            success_count += 1
        else:
            error_count += 1
    
    return {
        "results": results,
        "total_files": len(file_paths),
        "success_count": success_count,
        "error_count": error_count
    }

